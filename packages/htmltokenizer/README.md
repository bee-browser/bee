## Implementation Notes

There are reasons that our tokenizer implementation takes a pull-style approach,
instead of a push-style approach like a well-known SAX parser.

If we implement a tokenizer in a push-style approach, we have to define a trait
to handle tokens generated by the tokenizer like below:

```rust
trait TokenHandler {
    fn handle_start_tag(&mut self, tag: &Tag);
    fn handle_end_tag(&mut self, tag: &Tag);
}
```

If an HTML document has a `script` element,

```html
<script>
// div#b will be inserted before div#a.
document.write('<div id="b"></div>');
</script>
<div id="a"></div>
```

The tokenization must be blocked until the JavaScript execution is finished.
This means that the JavaScript must be executed during the `handle_end_tag()`
function call.

A JavaScript execution may take a long time.  So, the `handle_end_tag()` should
be an `async` function.  This may cause a performance issue because nested
`async` function calls cause overhead.

### document.write

we have to support multiple `document.write()` calls like below:

```html
<script>
document.write('<div id="');
document.write('b"></div>');
</script>
```

In order to parse the HTML fragments properly, we have to parse them by using
the same parser.

## Third-Party Resources

* src/charref/entities.json
  * https://html.spec.whatwg.org/entities.json
* src/html5libtests/*.test
  * https://github.com/html5lib/html5lib-tests/tree/master/tokenizer
